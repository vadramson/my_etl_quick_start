{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0214b13a-eb51-4a5b-a631-cc250ff17825",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adaf56f4-cb93-4824-b7f6-5c9681d12f25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/snowflake/connector/options.py:103: UserWarning: You have an incompatible version of 'pyarrow' installed (8.0.0), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, col, lit, array, when\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import json\n",
    "import snowflake.connector\n",
    "import os\n",
    "from datetime import datetime\n",
    "import hmac \n",
    "from pyspark.sql import functions as F\n",
    "from cryptography.fernet import Fernet\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f88401-3684-41c7-ac56-075780ffda0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Loading Credentials, Creating Spark Context, and Creating Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e70148c3-dcd0-40ec-a90a-03c740fcd231",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load configuration properties from the Congig file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd717ba-30e8-4a56-aa4a-1e26f0288d7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = {'user': 'user',\n",
    " 'password': 'password',\n",
    " 'account': 'account',\n",
    " 'warehouse': 'warehouse',\n",
    " 'database': 'database',\n",
    " 'schema': 'schema',\n",
    " 'role': 'role',\n",
    " 'db_host_src_pg': 'db_host_src_pg',\n",
    " 'user_src_pg': 'user_src_pg',\n",
    " 'password_src_pg': 'password_src_pg',\n",
    " 'database_src_pg': 'database_src_pg',\n",
    " 'username_mail': 'username_mail',\n",
    " 'password_mail': 'password_mail',\n",
    " 'enc_key': 'enc_key'}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e3c3857-4631-4fc9-af07-4d94c3a116c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading connection credentials from Databricks secrets into config dictionary\n",
    "\n",
    "for k,v in config.items():\n",
    "    #print(k,v)\n",
    "    config[k] = dbutils.secrets.get(scope=\"snow_cred\", key= k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992548c9-a34f-458a-8f64-917dda9404c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6645fd6e-db53-4695-bc8d-cd4d9357f074",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Set up SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237ff822-2052-44e5-ba8f-fca8b9a861ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"PostgreSQL_ETL\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .set(\"spark.jars.packages\" ,\n",
    "         \"net.snowflake:snowflake-jdbc:3.13.3,net.snowflake:spark-snowflake_2.12:2.9.0-spark_3.0\") \\\n",
    "    .set(\"spark.driver.extraClassPath\" , \"postgresql-42.2.24.jar\")\\\n",
    "    .set(\"spark.driver.memory\" , \"15g\") \\\n",
    "    .set(\"spark.executor.memory\" , \"15g\") \\\n",
    "    #.set(\"spark.driver.maxResultSize\" , \"6g\")\\\n",
    "    \n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19aad0a8-a6ed-4a66-a866-88794c4744cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define connection properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c723c18-83a0-437d-b6ee-3bfe56f88082",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PostgreSQL Properties\n",
    "fetch_size = 100000\n",
    "props = {\n",
    "    \"user\": config['user_src_pg'],\n",
    "    \"password\": config['password_src_pg'],\n",
    "    \"ssl\": \"true\",\n",
    "    \"sslmode\": \"require\", # verify-ca\n",
    "    \"sslrootcert\": \"/dbfs/FileStore/shared_uploads/some_directory/certs/root.crt\",\n",
    "    \"sslcert\": \"/dbfs/FileStore/shared_uploads/some_directory/certs/postgresql.crt\",\n",
    "    \"sslkey\": \"/dbfs/FileStore/shared_uploads/some_directory/certs/postgresql.unprotected.pk8\",\n",
    "    \"fetchsize\": str(fetch_size)\n",
    "    }\n",
    "\n",
    "# Define database url\n",
    "url = \"jdbc:postgresql://\" + config['db_host_src_pg'] + \":5432/\"+config['database_src_pg'] \n",
    "\n",
    "\n",
    "# Snowflake Properties\n",
    "\n",
    "snow_options = {\n",
    "    \"sfURL\": f\"{config['account']}.snowflakecomputing.com\" ,\n",
    "    \"sfUser\": config['user'] ,\n",
    "    \"sfPassword\": config['password'] ,\n",
    "    \"sfDatabase\": config['database'] ,\n",
    "    \"sfSchema\": config['schema'] ,\n",
    "    \"sfWarehouse\": config['warehouse'] ,\n",
    "    \"sfRole\": config['role'],\n",
    "    \"sfUseStagingTable\": \"true\",\n",
    "    \"sfTemporaryStage\": \"SOME_STAGE\",\n",
    "    \"sfPartitions\": \"8\",\n",
    "    \"sfBatchSize\": \"100000\",  # Adjust the batch size as needed\n",
    "    \"sfMaxConcurrency\": \"8\", # Adjust the maximum concurrency as needed\n",
    "    \"sfCopyOptions\" : \"FILE_FORMAT = 'CSV_FORMAT', COMPRESSION = 'AUTO', SKIP_HEADER = 1\"\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Establish a connection to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user = config['user'],\n",
    "    password = config['password'],\n",
    "    account= config['account'],\n",
    "    warehouse= config['warehouse'],\n",
    "    database= config['database'],\n",
    "    schema= config['schema']\n",
    ")\n",
    "\n",
    "\n",
    "# A list of messeages to send to me\n",
    "\n",
    "msg_to_send = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9993163-2a9a-47a0-9c6c-69ad6cdb5c9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82af9f4d-7819-4655-8156-747992d1bd33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dummy_json_str = {\n",
    "    \"_id\": \"6138763984fe5172962f8147\",\n",
    "    \"balance\": \"$3,677.47\",\n",
    "    \"about\": \"Lorem ipsum Aliqua ad elit elit veniam in mollit officia\",\n",
    "    \"registered\": \"2018-05-04T02:28:05 -02:00\",\n",
    "    \"latitude\": 60.763774\n",
    "}\n",
    "\n",
    "dummy_json = json.dumps(dummy_json_str)\n",
    "dummy_array = ['Lorem Ipsum']#dummy_json\n",
    "dummy_array = array(lit(dummy_array[0]))\n",
    "\n",
    "dummy_mail = 'dummy@mail.com'\n",
    "dummy_string = 'Lorem ipsum'\n",
    "dummy_phone = '+000000000000'\n",
    "dummy_ip = '127.0.0.1'\n",
    "dummy_date = '1660-01-01'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3087f004-181b-4967-b6e3-ec27ea5bda5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Main Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d72f6efb-c649-4d6e-a1a9-e4f099511744",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34107e82-6388-4485-aea9-2d8530577228",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if text is an email and encrypt the data\n",
    "\n",
    "\n",
    "def split_email(email):\n",
    "    parts = email.split('@', 1)\n",
    "    return parts[0], \"@\" + parts[1]\n",
    "\n",
    "def is_valid_email(email):\n",
    "    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    match = re.match(email_pattern, email)\n",
    "    return bool(match)\n",
    "\n",
    "def hash_value(text: str) -> str:\n",
    "    if not text:\n",
    "        text = 'null'\n",
    "\n",
    "    if is_valid_email(text):\n",
    "        username, domain = split_email(text)\n",
    "        return hashlib.sha256((username.encode() + config['enc_key'].encode())).hexdigest() + domain\n",
    "    else:\n",
    "        return hashlib.sha256((text.encode() + config['enc_key'].encode())).hexdigest()\n",
    "\n",
    "encrypt_data_udf = udf(hash_value, StringType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a646b81-13b7-43ce-9db2-dc576d35f542",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Anonymise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf448b1b-94e9-47d1-a5a2-3ced9af2a697",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def anonymise(tab : str, pg_df: 'DataFrame') -> 'DataFrame':\n",
    "    if tab == 'some_table__1':\n",
    "        pg_df = pg_df.withColumn(\"answers\", lit(dummy_array))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__2':\n",
    "        pg_df = pg_df.withColumn(\"json\", lit(dummy_array))\n",
    "        pg_df = pg_df.withColumn(\"whitelisted_ibans\", lit(dummy_array))\n",
    "        pg_df = pg_df.withColumn(\"balance\", lit(dummy_array))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__3':\n",
    "        pg_df = pg_df.withColumn(\"sender\", encrypt_data_udf(col(\"sender\")))\n",
    "        pg_df = pg_df.withColumn(\"recipients\", encrypt_data_udf(col(\"recipients\")))\n",
    "        pg_df = pg_df.withColumn(\"html\", encrypt_data_udf(col(\"html\")))\n",
    "        pg_df = pg_df.withColumn(\"subject\", encrypt_data_udf(col(\"subject\")))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__4':\n",
    "        pg_df = pg_df.withColumn(\"creation_ip\", encrypt_data_udf(col(\"creation_ip\")))\n",
    "        pg_df = pg_df.withColumn(\"validation_ip\", encrypt_data_udf(col(\"validation_ip\")))\n",
    "        pg_df = pg_df.withColumn(\"device_id\", encrypt_data_udf(col(\"device_id\")))\n",
    "        pg_df = pg_df.withColumn(\"phone\", lit(dummy_phone))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__5':\n",
    "        pg_df = pg_df.withColumn(\"name\", encrypt_data_udf(col(\"name\")))\n",
    "        pg_df = pg_df.withColumn(\"email\", encrypt_data_udf(col(\"email\")))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__6':\n",
    "        pg_df = pg_df.withColumn(\"license_plate\", encrypt_data_udf(col(\"license_plate\")))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__7':\n",
    "        pg_df = pg_df.withColumn(\"name\", encrypt_data_udf(col(\"name\")))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__8':\n",
    "        pg_df = pg_df.withColumn(\"json\", lit(dummy_json))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__9':\n",
    "        pg_df = pg_df.withColumn(\"body\", encrypt_data_udf(col(\"body\")))\n",
    "        pg_df = pg_df.withColumn(\"service\", lit(dummy_phone))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__10':\n",
    "        pg_df = pg_df.withColumn(\"stripe_source\", lit(dummy_json))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__11':        \n",
    "        pg_df = pg_df.withColumn(\"email\", encrypt_data_udf(col(\"email\")))\n",
    "        pg_df = pg_df.withColumn(\"firstname\", encrypt_data_udf(col(\"firstname\")))\n",
    "        pg_df = pg_df.withColumn(\"lastname\", encrypt_data_udf(col(\"lastname\")))\n",
    "        pg_df = pg_df.withColumn(\"current_sign_in_ip\", encrypt_data_udf(col(\"current_sign_in_ip\")))\n",
    "        pg_df = pg_df.withColumn(\"last_sign_in_ip\", encrypt_data_udf(col(\"last_sign_in_ip\")))\n",
    "        pg_df = pg_df.withColumn(\"birthdate\", lit(dummy_date))  \n",
    "        return pg_df    \n",
    "    elif tab == 'some_table__12':\n",
    "        pg_df = pg_df.withColumn(\"holder\", encrypt_data_udf(col(\"holder\")))\n",
    "        pg_df = pg_df.withColumn(\"bic\", lit(dummy_json))\n",
    "        pg_df = pg_df.withColumn(\"iban\", lit(dummy_json))\n",
    "        pg_df = pg_df.withColumn(\"unique_mandate_reference\", encrypt_data_udf(col(\"unique_mandate_reference\")))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__13':\n",
    "        ecnrypt_my_address = ['User', 'BeneficialOwner','CardShipping']\n",
    "\n",
    "        pg_df = pg_df.withColumn(\"street\", when(col(\"addressable_type\").isin(ecnrypt_my_address), encrypt_data_udf(col(\"street\"))).otherwise(col(\"street\")))\n",
    "        pg_df = pg_df.withColumn(\"city\", when(col(\"addressable_type\").isin(ecnrypt_my_address), encrypt_data_udf(col(\"city\"))).otherwise(col(\"city\")))\n",
    "        pg_df = pg_df.withColumn(\"postcode\", when(col(\"addressable_type\").isin(ecnrypt_my_address), encrypt_data_udf(col(\"postcode\"))).otherwise(col(\"postcode\")))\n",
    "        pg_df = pg_df.withColumn(\"phone\", lit(dummy_phone))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__14':\n",
    "        pg_df = pg_df.withColumn(\"firstname\", encrypt_data_udf(col(\"firstname\")))\n",
    "        pg_df = pg_df.withColumn(\"lastname\", encrypt_data_udf(col(\"lastname\")))\n",
    "        pg_df = pg_df.withColumn(\"middle_names\", encrypt_data_udf(col(\"middle_names\")))\n",
    "        pg_df = pg_df.withColumn(\"birthdate\", lit(dummy_date))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__15':\n",
    "        pg_df = pg_df.withColumn(\"current_phone\", lit(dummy_phone))\n",
    "        pg_df = pg_df.withColumn(\"next_phone\", lit(dummy_phone))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__16':\n",
    "        pg_df = pg_df.withColumn(\"name\", encrypt_data_udf(col(\"name\")))\n",
    "        return pg_df\n",
    "    elif tab == 'some_table__17':\n",
    "        pg_df = pg_df.withColumn(\"email\", encrypt_data_udf(col(\"email\")))\n",
    "        pg_df = pg_df.withColumn(\"sendgrid_data\", lit(dummy_json))\n",
    "        return pg_df\n",
    "    else:\n",
    "        return pg_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "250f8abb-f7f2-44a5-ba7b-bb405bc30163",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reading data from tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfdabc6e-a6da-4da1-b258-7d4f661a9ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment below for DEV\n",
    "\n",
    "\"\"\"\n",
    "snow_options[\"sfSchema\"] = \"SOME_SCHEMA\"\n",
    "\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user = config['user'],\n",
    "    password = config['password'],\n",
    "    account= config['account'],\n",
    "    warehouse= config['warehouse'],\n",
    "    database= config['database'],\n",
    "    schema= \"PUBLIC_CLONE\"#config['schema']\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79588ebc-0303-41c2-949b-8afb48777cfb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Reading data from source table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9328a82c-6a67-4693-9d51-e539919120b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Data from PostgreSQL Database table\n",
    "\n",
    "def read_src_pg_table(url: str, query: str, properties: dict) -> 'DataFrame':\n",
    "    try:\n",
    "        # Read table into a dataframe\n",
    "        pg_table_df = spark.read.jdbc(url=url, table=query, properties=properties)\n",
    "        return pg_table_df\n",
    "    except Exception as e:\n",
    "        print(\"Error occured -> \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cfa958c-b210-40d1-be49-933e2c2b5977",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Reading data from target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f5e728-f7fb-4dfd-9cff-8bc042b02ce3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Data from Snowflake Database table\n",
    "\n",
    "def read_tgt_snw_table(query: str, snow_options: dict) -> 'DataFrame':\n",
    "    try:\n",
    "        # Read table into a dataframe\n",
    "        snow_table_df = spark.read.format(\"snowflake\").options(**snow_options).option(\"query\", query).load()\n",
    "        return snow_table_df\n",
    "    except Exception as e:\n",
    "        print(\"Error occured -> \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44f7acfe-d996-4d65-acb6-b0522aea08a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Get source database tables to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4566313-c55f-4428-b993-4029b3a14b94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all database tables to migrate\n",
    "\n",
    "def get_all_tabs_to_migrate(url: str, get_all_tables_pg: str, pg_options: dict) -> list:\n",
    "    #get_all_tables_pg = \"(SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_type = 'BASE TABLE' order by table_name asc) all_tabs\"\n",
    "    all_pg_tabs = read_src_pg_table(url, get_all_tables_pg, pg_options)\n",
    "    all_pg_tabs = all_pg_tabs.select(\"table_name\").collect()\n",
    "    return [row[\"table_name\"] for row in all_pg_tabs]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b064f748-238a-44f5-8262-66d1725eb115",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Get target database tables to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb1bd08-fff9-4b3d-8507-54e9a8aabcac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all target database tables\n",
    "\n",
    "def get_all_tgt_tabs_to_migrate(snow_options: dict) -> list:\n",
    "    get_all_tar_tables_snw = f\"(SELECT DISTINCT LOWER(TABLE_NAME) as TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{snow_options['sfSchema']}' AND TABLE_CATALOG = '{snow_options['sfDatabase']}' AND TABLE_TYPE = 'BASE TABLE' AND LOWER(TABLE_NAME) NOT IN ('job_accuracy', 'migration_monitoring', 'km_data', 'john_paul_sent', 'kpis', 'all_pub_tables', 'mastercard_interchanges_calculated', 'special_tables', 'job_execution_time', 'public_tables', 'risky_mcc') ORDER BY LOWER(TABLE_NAME) ASC) all_tgt_tabs\"\n",
    "    all_snw_tabs = read_tgt_snw_table(get_all_tar_tables_snw, snow_options)\n",
    "    all_snw_tabs = all_snw_tabs.select(\"TABLE_NAME\").collect()\n",
    "    return [row[\"TABLE_NAME\"] for row in all_snw_tabs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f522ad03-93da-4f12-952f-bd0f57a80d4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Schema change detection and handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6decc4e-fc1a-43bd-9ad1-c6134b1d6f2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Schema change detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d65877d-4894-4a41-8e3e-e6fe747fff43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_schema_difference(table_to_check: str) -> bool:\n",
    "    chng_str = \"\"\n",
    "    \n",
    "    # Processing Source (Postgres) table\n",
    "    src_pg_query = f\"(SELECT * FROM {table_to_check} LIMIT 1) {table_to_check}\"\n",
    "    src_pg_df = read_src_pg_table(url, src_pg_query, props)\n",
    "    \n",
    "\n",
    "    # Get column metadata from Source (Postgres) table\n",
    "    src_pg_metadata = set([(row.name.lower(), str(row.dataType), row.nullable) for row in src_pg_df.schema])\n",
    "    #print(f\"\\nsrc_pg_metadata -> {src_pg_metadata}\\n\")\n",
    "\n",
    "\n",
    "    # Processing Target (Snowflake) table\n",
    "    tar_snow_query = f\"(SELECT * FROM {table_to_check} LIMIT 1) {table_to_check}\"\n",
    "    tar_snow_df = read_tgt_snw_table(tar_snow_query, snow_options)\n",
    "    #print(f\"len SRC -> {len(src_pg_df.columns)} and len TGT -> {len(tar_snow_df.columns)}\")\n",
    "    #print(tar_snow_df.printSchema())\n",
    "    #print(f\"\\ntar_snow_df {tar_snow_df} {type(tar_snow_df)} {list(tar_snow_df)}\")\n",
    "\n",
    "\n",
    "    # Get column metadata from Target (Snowflake) table\n",
    "    tar_snow_metadata = set([(row.name.lower(), str(row.dataType), row.nullable) for row in tar_snow_df.schema])\n",
    "    #print(f\"\\ntar_snow_metadata -> {tar_snow_metadata}\\n\")\n",
    "\n",
    "\n",
    "    # Definition of data type mapping between Postgres and Snowflake\n",
    "    data_type_mapping = {\n",
    "        'LongType()': 'DecimalType(38,0)',\n",
    "        'bigint': 'DecimalType(38,0)',\n",
    "        'IntegerType()': 'DecimalType(38,0)',\n",
    "        'ArrayType(StringType(), True)': 'StringType()',\n",
    "        'ShortType()': 'DecimalType(38,0)'\n",
    "    }\n",
    "\n",
    "    # Apply data type mapping to Source (Postgres) metadata\n",
    "    mapped_src_pg_metadata = set(list([(column_name.lower(), data_type_mapping.get(data_type, data_type), is_nullable) for column_name, data_type, is_nullable in src_pg_metadata]))    \n",
    "    #print(f\"mapped_src_pg_metadata --> {mapped_src_pg_metadata}\")\n",
    "\n",
    "    # Apply nullable mapping to Source (Postgres) metadata\n",
    "    \n",
    "\n",
    "    # Compare schemas\n",
    "    schema_diff =  mapped_src_pg_metadata - tar_snow_metadata\n",
    "    \n",
    "    print(f\"\\n\\nDifference {schema_diff}\")\n",
    "    #print(f\"len SRC -> {len(src_pg_df.columns)} and len TGT -> {len(tar_snow_df.columns)}\")\n",
    "\n",
    "    if len(schema_diff) > 0 and len(src_pg_df.columns) != len(tar_snow_df.columns): # check if there's no difference in the columns and in the length of columns\n",
    "        for column in schema_diff:\n",
    "            column_name, data_type, is_nullable = column\n",
    "            chng_str += f\"Column: {column_name}, Data Type: {data_type}, Nullable: {is_nullable}\\n\"\n",
    "        \n",
    "        chng_str = f\"Database Table {table_to_check}\\n\" + chng_str\n",
    "        msg_to_send.append(chng_str)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "973c04b8-1f9e-40f7-a50c-b1a4c177ead7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Schema change handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db827f6f-ba6a-4020-b47d-c7e72b7eab23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def handle_schema_change(data_df: 'DataFrame', table_name: str, versioning_schema: str) -> None:\n",
    "    try:\n",
    "        # Get current timestamp\n",
    "        current_timestamp = str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        temp_new_table = table_name+'__'+current_timestamp\n",
    "        print(temp_new_table)\n",
    "\n",
    "        # Save table with new name\n",
    "        data_df.write.format(\"snowflake\").options(**snow_options).option(\"dbtable\", temp_new_table).mode(\"append\").save() # \"overwrite\" \"append\"\n",
    "\n",
    "        # Updates: \n",
    "                # -> Move old table to alternate schema and rename it by appending the current_timestamp of the new table to its name\n",
    "                # -> Rename the new table to the name of the old table by stripping the current_timestamp from it\n",
    "\n",
    "        # Start a transaction\n",
    "        conn.cursor().execute(\"BEGIN TRANSACTION\")\n",
    "\n",
    "        try:\n",
    "            # Send old table to temp schema and rename it\n",
    "            conn.cursor().execute(f\"ALTER TABLE {snow_options['sfDatabase']}.{snow_options['sfSchema']}.{table_name} RENAME TO {snow_options['sfDatabase']}.{versioning_schema}.{temp_new_table}\")\n",
    "\n",
    "            # Rename new table to old table\n",
    "            conn.cursor().execute(f\"ALTER TABLE {snow_options['sfDatabase']}.{snow_options['sfSchema']}.{temp_new_table} RENAME TO {snow_options['sfDatabase']}.{snow_options['sfSchema']}.{table_name}\")\n",
    "\n",
    "            # Commit the transaction\n",
    "            conn.cursor().execute(\"COMMIT\")\n",
    "            msg = f\"Schema change detected and new table created at {temp_new_table}\"\n",
    "\n",
    "            msg_to_send.append(msg)\n",
    "        except Exception as e:\n",
    "            # Rollback the transaction in case of any error\n",
    "            conn.cursor().execute(\"ROLLBACK\")\n",
    "            print(\"Rolling Back...\\nError occurred -> \", e)    \n",
    "\n",
    "\n",
    "        #return saved_df\n",
    "    except Exception as e:\n",
    "        print(\"Error occured -> \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02741405-d895-44bd-a634-8d979b65c474",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d4e99e1-3113-477e-973a-69972a50e316",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Saving Data to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94fd700b-1671-4315-a970-1140d0c91546",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### UPSERT target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c375aff-29a3-43d5-bf3a-0441d5b03ebc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method essentially updates (MERGE) the target table\n",
    "\"\"\"\n",
    "\n",
    "def upsert_target_table(tab: str, merge_key: str = None) -> bool:\n",
    "\n",
    "    if merge_key is None:\n",
    "        merge_key = 'id'\n",
    "\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"BEGIN TRANSACTION\")\n",
    "\n",
    "    try:\n",
    "        ## Get the column names of the target table\n",
    "        target_columns_query = f\"SHOW COLUMNS IN {snow_options['sfDatabase']}.{snow_options['sfSchema']}.{tab}\" \n",
    "        cursor.execute(target_columns_query)\n",
    "        column_names = [column[2] for column in cursor.fetchall()]\n",
    "\n",
    "        # Generate the column mappings for the MERGE query, handling reserved keywords\n",
    "        column_mappings = \", \".join([f\"\\\"target\\\".\\\"{col}\\\" = \\\"source\\\".\\\"{col}\\\"\" for col in column_names])\n",
    "        #print(f\"column_mappings {column_mappings}\")\n",
    "\n",
    "        # Perform the MERGE operation to insert new rows or update existing rows\n",
    "        #print(f\"Upserting.... snow_options['sfSchema'] {snow_options['sfSchema']}\")\n",
    "        merge_query = (\n",
    "            \"MERGE INTO \\\"\" + snow_options['sfDatabase']+\"\\\".\"+\"\\\"\" + snow_options['sfSchema']+\"\\\".\"+\"\\\"\"+str(tab).upper() + \"\\\" AS \\\"target\\\" \"\n",
    "            \"USING \" + snow_options['sfDatabase']+\".\" + snow_options['sfSchema']+\".TEMP_TABLE AS \\\"source\\\" \"\n",
    "            \"ON \\\"target\\\".\\\"\" + merge_key + \"\\\" = \\\"source\\\".\\\"\" + merge_key + \"\\\" \"\n",
    "            \"WHEN MATCHED THEN \"\n",
    "            \"UPDATE SET \" + column_mappings + \" \"\n",
    "            \"WHEN NOT MATCHED THEN \"\n",
    "            \"INSERT (\" + \", \".join([\"\\\"\" + col + \"\\\"\" for col in column_names]) + \") \"\n",
    "            \"VALUES (\" + \", \".join([\"\\\"source\\\".\\\"\" + col + \"\\\"\" for col in column_names]) + \")\"\n",
    "        )\n",
    "        #print(f\"\\nQuery to be executed {merge_query}\")\n",
    "        cursor.execute(merge_query)\n",
    "        #print(f\"\\nQuery executed\")\n",
    "        \n",
    "        # Delete the previously created TEMP_TABLE to free up the database\n",
    "        cursor.execute(f\"DROP TABLE {snow_options['sfDatabase']}.{snow_options['sfSchema']}.TEMP_TABLE\")\n",
    "\n",
    "        print(f\"\\nDrop table Query executed\")\n",
    "\n",
    "        # Commit the changes and close the connection\n",
    "        cursor.execute(\"COMMIT\")  \n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # Rollback the transaction in case of any error    \n",
    "        msg_to_send.append(f\"Could not Merge UPDATE table {tab} because of the following error\\n{e}\\n, and consequently the table was not migrated.\\n Go take a look\")\n",
    "        cursor.execute(\"ROLLBACK\")\n",
    "        print(\"Rolling Back...\\nError occurred -> \", e)  \n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f32216d7-f609-4a1b-9c85-34787c01d585",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Saving data to target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80378f8-1e52-44cb-9476-b496718f352c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save Data to Snowflake Database table\n",
    "\n",
    "def load_tgt_snw_table(data_df: 'DataFrame', database_table: str, snow_options: dict, write_mode: str = None, updating: str = None) -> 'DataFrame':\n",
    "    try:\n",
    "        old_tab = database_table # Keep the table name just in case\n",
    "\n",
    "        # Default write method\n",
    "        if write_mode is None:\n",
    "            write_mode = \"append\"\n",
    "        \n",
    "        # If updating, save data instead in a TEMP_TABLE for later MERGE operation and then change write mode to overite\n",
    "        if updating is not None:\n",
    "            database_table = \"TEMP_TABLE\"\n",
    "            write_mode = \"overwrite\"\n",
    "\n",
    "        # Read table into a dataframe\n",
    "        #saved_df = \n",
    "        data_df.write.format(\"snowflake\").options(**snow_options).option(\"dbtable\", database_table).mode(f\"{write_mode}\").save()\n",
    "\n",
    "        if updating is not None:\n",
    "            if old_tab in ['mastercard_interchanges', 'mastercard_transaction_corrections']:\n",
    "                upsert_target_table(old_tab, 'MASTERCARD_TRANSACTION_ID')\n",
    "            else:\n",
    "                upsert_target_table(old_tab)\n",
    "\n",
    "        #return saved_df\n",
    "    except Exception as e:\n",
    "        print(\"Error occured -> \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4e6b9ab-6cdd-4033-9ecc-9b2cfc7a1a93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sending Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536739dd-0d5e-4a03-86cf-c9c8a8a738bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def send_mail(subject: str, message: str, any_link: str=None) -> None:\n",
    "    import smtplib\n",
    "    from email.mime.text import MIMEText\n",
    "    from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "    if any_link is None:\n",
    "        any_link = 'No link'\n",
    "\n",
    "    \n",
    "    username = config['username_mail'] \n",
    "    password = config['password_mail']\n",
    "    msg = MIMEMultipart('mixed')\n",
    "\n",
    "    sender = 'my_email@mail.com'\n",
    "    recipient = 'my_email@mail.com'\n",
    "\n",
    "    msg['Subject'] = subject \n",
    "    msg['From'] = sender\n",
    "    msg['To'] = recipient\n",
    "    \n",
    "    text_message = MIMEText(str(message))\n",
    "    html_message = MIMEText(f\"<br><b>File Located at:</b> <a href='{any_link}'>{any_link}</a>\", 'html')\n",
    "    msg.attach(text_message)\n",
    "    msg.attach(html_message)\n",
    "\n",
    "    mailServer = smtplib.SMTP('mail.smtp2go.com', 2525) # 8025, 587 and 25 can also be used.\n",
    "    mailServer.ehlo()\n",
    "    mailServer.starttls()\n",
    "    mailServer.ehlo()\n",
    "    mailServer.login(username, password)\n",
    "    mailServer.sendmail(sender, recipient, msg.as_string())\n",
    "    mailServer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17ff0b6-500a-43ad-8819-14743c05f80a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e5bcf2b-2526-4356-869c-8e70a356867c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Testing DB Connections and geting source and target tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76355dcf-3fc7-46de-a1ef-62cc1ad495f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Get list of tables to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46cf6f4a-fe23-4a5a-b7c7-62f842c25a51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tables -> 188 \n",
      "Source tables -> 204\n"
     ]
    }
   ],
   "source": [
    "get_all_tables_pg = \"(SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_type = 'BASE TABLE' order by table_name asc) all_tabs\"\n",
    "all_pg_tabs = get_all_tabs_to_migrate(url, get_all_tables_pg, props) # Get a list of all source tables from the source database\n",
    "len(all_pg_tabs), all_pg_tabs\n",
    "\n",
    "all_snw_tabs = get_all_tgt_tabs_to_migrate(snow_options)\n",
    "print(f\"Target tables -> {len(all_pg_tabs)} \\nSource tables -> {len(all_snw_tabs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81283837-cc20-4866-b727-ba272a49cd3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 7, 148)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_up_query = \"(SELECT table_name FROM information_schema.columns WHERE table_schema = 'public' AND column_name IN ('created_at', 'updated_at', 'id') GROUP BY table_name HAVING COUNT(DISTINCT column_name) = 3) created_at__updated_at\"\n",
    "\n",
    "get_all_tables_pg = \"(SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_type = 'BASE TABLE' order by table_name asc) all_tabs\"\n",
    "\n",
    "only_created_query = \"(SELECT table_name FROM information_schema.columns WHERE table_schema = 'public' AND column_name IN ('created_at', 'id') GROUP BY table_name HAVING COUNT(DISTINCT column_name) = 2) only_created_at\"\n",
    "\n",
    "only_updated_query = \"(SELECT table_name FROM information_schema.columns WHERE table_schema = 'public' AND column_name IN ('id') GROUP BY table_name HAVING COUNT(DISTINCT column_name) = 1) only_updated_at\"\n",
    "\n",
    "all_source_tables = get_all_tabs_to_migrate(url, get_all_tables_pg, props) # Get a list of all source tables from the source database\n",
    "\n",
    "\n",
    "created_at__updated_at = get_all_tabs_to_migrate(url, cr_up_query, props) # Get a list of all source tables containing columns 'created_at', 'updated_at', and 'id'\n",
    "\n",
    "all_created_at = get_all_tabs_to_migrate(url, only_created_query, props) # Get a list of all source tables containing at least the columns 'created_at', and 'id'\n",
    "just_created_at = list(set(all_created_at) - set(created_at__updated_at)) # Get a list of all source tables containing only 'created_at', and 'id' columns\n",
    "\n",
    "only_updated_at = get_all_tabs_to_migrate(url, only_updated_query, props)\n",
    "only_updated_at = list(set(only_updated_at) - set(created_at__updated_at))\n",
    "\n",
    "len(all_created_at), len(just_created_at), len(created_at__updated_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d333e465-889e-4ee3-b9fa-02e1d82a67c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Core Migration method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27f5b4e2-039b-4749-9dd3-dc429b5ed38d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "933c717c-05de-4fd4-a92f-052fc19b2205",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if the tables in the add_to_created_at list exists in the all_pg_tabs(ALL SOURCE TABLES) list.\n",
    "And if yes, append the tables to the all_created_at list and proceed with migrating the tables in the updates all_created_at list.\n",
    "\"\"\"\n",
    "\n",
    "add_to_created_at = ['loyaltek_text_messages', 'account_movements', 'payment_errors', 'ahoy_messages']\n",
    "\n",
    "\n",
    "[all_created_at.append(i) for i in add_to_created_at if i in all_pg_tabs]\n",
    "\n",
    "date_col = {\n",
    "            'loyaltek_text_messages': 'at',\n",
    "            'account_movements': 'transaction_date',\n",
    "            'payment_errors': 'at',\n",
    "            'ahoy_messages': 'sent_at'\n",
    "            }\n",
    "all_created_at = [i for i in all_created_at if i in all_source_tables]\n",
    "all_created_at = sorted(all_created_at)\n",
    "\n",
    "cnt = 0\n",
    "for tab in all_created_at:\n",
    "    try:\n",
    "        print(\"\\nmigrating table \", tab, \"...\\n\")\n",
    "        max_col = 'created_at'\n",
    "        cnt += 1\n",
    "        if tab in add_to_created_at:\n",
    "            max_col = date_col[tab]\n",
    "\n",
    "        if tab not in all_snw_tabs:        \n",
    "            pg_query = f\"(SELECT * FROM {tab} ORDER BY {max_col} ASC ) AS {tab}\"\n",
    "            pg_df = read_src_pg_table(url, pg_query, props)\n",
    "            if len(pg_df.head(1)) == 0:\n",
    "                continue\n",
    "            pg_df.cache()  # Cache the DataFrame in memory\n",
    "            load_tgt_snw_table(pg_df, tab, snow_options, \"overwrite\")\n",
    "            pg_df.unpersist()\n",
    "            msg_to_send.append(f\"New table {tab} migrated\")\n",
    "            print(\"\\nmigrated table \", tab, \"\\n\")\n",
    "            continue\n",
    "\n",
    "        if check_schema_difference(tab):\n",
    "            print('handle')\n",
    "            pg_query = f\"(SELECT * FROM {tab} ) AS {tab}\"\n",
    "            pg_df = read_src_pg_table(url, pg_query, props)\n",
    "            if len(pg_df.head(1)) == 0:\n",
    "                continue\n",
    "            pg_df.cache()  # Cache the DataFrame in memory\n",
    "            print(pg_query)\n",
    "            handle_schema_change(pg_df, tab, 'TEMP_HOLD')\n",
    "            pg_df.unpersist()\n",
    "            print(\"\\nmigrated table \", tab, \"\\n\")\n",
    "        else:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(f\"SELECT MAX({max_col}) FROM {tab}\")\n",
    "            max_val = cursor.fetchone()[0]\n",
    "            print('max_date ', max_val)\n",
    "\n",
    "            if max_val:\n",
    "                # Extension query: updated_at\n",
    "                extension = f\" OR updated_at >= '{max_val}' \"\n",
    "                if tab in just_created_at or tab in add_to_created_at:\n",
    "                    extension = \"\"\n",
    "\n",
    "                pg_query = f\"(SELECT * FROM {tab} WHERE {max_col} >= '{max_val}' {extension} ORDER BY {max_col} ASC ) AS {tab}\"\n",
    "                print('next saved ')\n",
    "                pg_df = read_src_pg_table(url, pg_query, props)\n",
    "                pg_df.cache()  # Cache the DataFrame in memory\n",
    "\n",
    "                # Anonymise Data\n",
    "                pg_df = anonymise(tab, pg_df)\n",
    "            \n",
    "                #display(pg_df)\n",
    "\n",
    "                load_tgt_snw_table(pg_df, tab, snow_options, \"overwrite\", \"update\")\n",
    "                pg_df.unpersist()\n",
    "                print(\"\\nmigrated table \", tab, \"\\n\")\n",
    "            else:\n",
    "                if max_val is not None:\n",
    "                    msg_to_send.append(f\"Table {tab} could not be migrated because it has no specific DATE to filter source rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Core Migration Error -> \\n{e}\")\n",
    "        msg_to_send.append(f\"Error while attempting to migrate the table {tab}\")\n",
    "        continue\n",
    "print(f\"Nummber of tables to migrate {len(all_created_at)}, number of tables actualy migrated {cnt} \")\n",
    "\n",
    "if cnt != len(all_created_at):\n",
    "    msg_to_send.append(f\"Nummber of tables to migrate {len(all_created_at)}, number of tables actualy migrated {cnt} \")         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce0d4a6a-2d9b-44bb-9352-8dcd88d595e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "476a8057-a18e-424f-92ad-c534eef4dde9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#print(msg_to_send)\n",
    "if len(msg_to_send) > 0:\n",
    "    any_link = 'https://databricks_notebook_link'\n",
    "    sbj = 'PostgreSQL Migration - Logs from Databrick! '\n",
    "    final_msg = ''.join([f\"{i}\\n\" for i in msg_to_send])\n",
    "    send_mail(sbj, final_msg, any_link)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8b403a-9ba9-4cf1-b87e-6c3a32413753",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_msg = []\n",
    "msg_to_send = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef001773-0263-49f7-a7e0-3246ff9eae36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spacial tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e3edea-3838-4fcd-bb8a-ce148b910d26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "none_int_id = ['attendees', 'email_validations', 'vat_rates', 'expense_attendee_guesses', 'sso_login_domains', 'active_storage_variant_records']\n",
    "\n",
    "always_overwrite = ['card_patterns_expense_categories', 'cards_tags', 'billing_invoices_stripe_payments', 'sso_email_domains', 'card_mccs', 'expense_analytical_axes', 'expense_analytic_codes', 'supplier_mastercard_merchants', 'archival_transfers_receipts', 'schema_migrations', 'ar_internal_metadata']\n",
    "\n",
    "using_id = ['consents', 'expense_analytic_code_guesses', 'expense_business_code_guesses', 'expense_vats', 'invoice_lines', 'invoices', 'mastercard_transaction_corrections', 'mastercard_interchanges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddaa9ba6-eb17-4196-8170-e54d1d08c92c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Spacial tables -> Using IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1b9e11-8f5a-48e3-ae3a-3e01935af7fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if the tables in the none_int_id list exists in the all_pg_tabs(ALL SOURCE TABLES) list.\n",
    "And if yes, append the tables to the using_id list and proceed with migrating the tables in the using_id list.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "[using_id.append(i) for i in none_int_id if i in all_pg_tabs]\n",
    "\n",
    "using_id = [i for i in using_id if i in all_source_tables]\n",
    "using_id = sorted(using_id)\n",
    "\n",
    "for tab in using_id:\n",
    "    try:\n",
    "        print(\"\\nmigrating table \", tab, \"...\\n\")\n",
    "        max_col = 'id'\n",
    "        int_id = '::int'\n",
    "        if tab in none_int_id:\n",
    "            int_id = ''\n",
    "            print(f\"None int it \")\n",
    "        if tab in ['mastercard_interchanges', 'mastercard_transaction_corrections']:\n",
    "            max_col = 'mastercard_transaction_id'\n",
    "            \n",
    "        if tab not in all_snw_tabs:        \n",
    "            pg_query = f\"(SELECT * FROM {tab} ORDER BY {max_col}{int_id} ASC ) AS {tab}\"\n",
    "            pg_df = read_src_pg_table(url, pg_query, props)\n",
    "            if len(pg_df.head(1)) == 0:\n",
    "                continue\n",
    "            pg_df.cache()  # Cache the DataFrame in memory\n",
    "            load_tgt_snw_table(pg_df, tab, snow_options, \"overwrite\")\n",
    "            pg_df.unpersist()\n",
    "            msg_to_send.append(f\"New table {tab} migrated\")\n",
    "            print(\"\\nmigrated table \", tab, \"\\n\")\n",
    "            continue\n",
    "\n",
    "        if check_schema_difference(tab):\n",
    "            print('handle')\n",
    "            pg_query = f\"(SELECT * FROM {tab} ORDER BY {max_col}{int_id} ASC ) AS {tab}\"\n",
    "            pg_df = read_src_pg_table(url, pg_query, props)\n",
    "            if len(pg_df.head(1)) == 0:\n",
    "                continue\n",
    "            pg_df.cache()  # Cache the DataFrame in memory\n",
    "            #print(pg_query)\n",
    "            handle_schema_change(pg_df, tab, 'TEMP_HOLD')\n",
    "            pg_df.unpersist()\n",
    "            print(\"\\nmigrated table \", tab, \"\\n\")\n",
    "        else:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(f\"SELECT MAX({max_col}{int_id} ) FROM {tab}\")\n",
    "            max_val = cursor.fetchone()[0]\n",
    "            print('max_val ', max_val)\n",
    "\n",
    "            if max_val:            \n",
    "                if tab in none_int_id:\n",
    "                    pg_query = f\"(SELECT * FROM {tab} WHERE {max_col}{int_id}  >= '{max_val}' ORDER BY {max_col}{int_id}  ASC ) AS {tab}\"\n",
    "                else:\n",
    "                    pg_query = f\"(SELECT * FROM {tab} WHERE {max_col}{int_id}  >= {max_val} ORDER BY {max_col}{int_id}  ASC ) AS {tab}\"\n",
    "\n",
    "                print('max ', pg_query)\n",
    "                pg_df = read_src_pg_table(url, pg_query, props)\n",
    "\n",
    "                # Anonymise Data\n",
    "                pg_df = anonymise(tab, pg_df)\n",
    "                pg_df.cache()  # Cache the DataFrame in memory\n",
    "                #display(pg_df)\n",
    "\n",
    "                load_tgt_snw_table(pg_df, tab, snow_options, \"overwrite\", \"update\")\n",
    "                pg_df.unpersist()\n",
    "                print(\"\\nmigrated table \", tab, \"\\n\")\n",
    "            else:\n",
    "                if max_val is not None:\n",
    "                    msg_to_send.append(f\"using_id :: Table {tab} could not be migrated because it has no specific DATE to filter source rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Using id Error -> \\n{e}\")\n",
    "\n",
    "\n",
    "\n",
    "#print(msg_to_send)\n",
    "if len(msg_to_send) > 0:\n",
    "    any_link = 'https://databricks_notebook_link'\n",
    "    sbj = 'Special Tables PostgreSQL Migration - Logs from Databrick! '\n",
    "    final_msg = ''.join([f\"{i}\\n\" for i in msg_to_send])\n",
    "    send_mail(sbj, final_msg, any_link)            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11abf566-9437-4859-8ecf-9e65d38c2c86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Spacial tables -> Always Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bded18e4-cd2c-45df-9f17-0d17028d94c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "always_overwrite = ['card_patterns_expense_categories', 'billing_invoice_lines', 'disbursements', 'cards_tags', 'billing_invoices_stripe_payments', 'sso_email_domains', 'card_mccs', 'expense_analytical_axes', 'expense_analytic_codes', 'supplier_mastercard_merchants', 'archival_transfers_receipts', 'schema_migrations', 'ar_internal_metadata']\n",
    "\n",
    "# Check if the tables in the always_overwrite list exists in all_pg_tabs(ALL SOURCE TABLES) and if yes, proceed with migrating the table\n",
    "\n",
    "for tab in [i for i in always_overwrite if i in all_pg_tabs]: \n",
    "    print(tab)\n",
    "    pg_query = f\"(SELECT * FROM {tab} ) AS {tab}\"    \n",
    "    pg_df = read_src_pg_table(url, pg_query, props)\n",
    "    pg_df.cache()  # Cache the DataFrame in memory\n",
    "    load_tgt_snw_table(pg_df, tab, snow_options, \"overwrite\",)\n",
    "    pg_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c6548e-9a6e-46f6-8e1f-d18c63b49aa4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### End Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed62ed6-661e-4602-be36-4b418642cd93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the code below after DEV\n",
    "#spark.stop()\n",
    "\n",
    "# Close the connection\n",
    "#cursor.close()\n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e53ce38e-efb8-4181-813e-3d9741b9ef5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DELETE ALL the cells after this one when you move to PROD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c191844-7544-4175-8377-70a05fe16328",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pg_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fad052c-8194-4a3a-8c78-ffa510844aad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "max_col = 'started_at'\n",
    "tab = 'SOME_TABLE'\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\"SELECT ID FROM {tab}\")\n",
    "original_list = cursor.fetchall()\n",
    "\n",
    "new_list = [item[0] for item in original_list]\n",
    "\n",
    "result_tuple = tuple(new_list)\n",
    "result_tuple\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df913147-5cd2-41a0-95d6-1e2c9c6302fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "pg_query = f\"(SELECT * FROM {tab} WHERE ID NOT IN {result_tuple}) AS {tab}\"    # pg_query = f\"(SELECT COUNT({max_col})::int AS num_rows_prod FROM {tab} ) AS {tab}\"    \n",
    "\n",
    "pg_df = read_src_pg_table(url, pg_query, props)\n",
    "display(pg_df)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a358584-d859-4132-994a-c218c99d8d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "d13e7d7b-f978-4073-a96f-094dc07ba626",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "6638cd47-8847-4803-ae0d-566341c948be",
     "origId": 2033082681461173,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Main_migration_Job",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
