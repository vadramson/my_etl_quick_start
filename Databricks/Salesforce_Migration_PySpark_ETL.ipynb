{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0214b13a-eb51-4a5b-a631-cc250ff17825",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adaf56f4-cb93-4824-b7f6-5c9681d12f25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyspark.sql import SparkSession\\nfrom simple_salesforce import Salesforce\\nfrom pyspark import SparkConf, SparkContext\\nfrom pyspark.sql.types import *\\nimport json\\nimport snowflake.connector\\nimport os\\nfrom datetime import datetime\\nimport hmac \\nimport hashlib\\nimport urllib.parse\\nfrom pyspark.sql import functions as F\\nfrom cryptography.fernet import Fernet\\nimport re\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from simple_salesforce import Salesforce\n",
    "from pyspark.sql.functions import to_timestamp, col, lit, array\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import snowflake.connector\n",
    "import os\n",
    "from datetime import datetime\n",
    "import hmac \n",
    "import hashlib\n",
    "import urllib.parse\n",
    "from pyspark.sql import functions as F\n",
    "from cryptography.fernet import Fernet\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from simple_salesforce import Salesforce\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import snowflake.connector\n",
    "import os\n",
    "from datetime import datetime\n",
    "import hmac \n",
    "import hashlib\n",
    "import urllib.parse\n",
    "from pyspark.sql import functions as F\n",
    "from cryptography.fernet import Fernet\n",
    "import re\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f88401-3684-41c7-ac56-075780ffda0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Loading Credentials, Creating Spark Context, and Creating Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e70148c3-dcd0-40ec-a90a-03c740fcd231",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load configuration properties from the Congig file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51f9436-8bc8-4091-9e28-ec9dd509f4b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = {'user': 'user',\n",
    " 'password': 'password',\n",
    " 'account': 'account',\n",
    " 'warehouse': 'warehouse',\n",
    " 'database': 'database',\n",
    " 'schema': 'schema',\n",
    " 'role': 'role',\n",
    " 'username_sf': 'username_sf',\n",
    " 'password_sf': 'password_sf',\n",
    " 'securityToken_sf': 'securityToken_sf',\n",
    " 'authEndPoint_sf': 'authEndPoint_sf',\n",
    " 'username_mail': 'username_mail',\n",
    " 'password_mail': 'password_mail',\n",
    " 'enc_key': 'enc_key'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cafe56fe-c215-4180-a53a-eba6016190da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading connection credentials from Databricks secrets into config dictionary\n",
    "\n",
    "for k,v in config.items():\n",
    "    #print(k,v)\n",
    "    config[k] = dbutils.secrets.get(scope=\"snow_salesforce_creds\", key= k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6645fd6e-db53-4695-bc8d-cd4d9357f074",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Set up SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237ff822-2052-44e5-ba8f-fca8b9a861ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Salesforce_ETL\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .set(\"spark.jars.packages\" ,\n",
    "         \"net.snowflake:snowflake-jdbc:3.13.3,net.snowflake:spark-snowflake_2.12:2.9.0-spark_3.0\") \\\n",
    "    .set(\"spark.driver.memory\" , \"10g\") \\\n",
    "    .set(\"spark.executor.memory\" , \"6g\") \\\n",
    "    .set(\"spark.driver.maxResultSize\" , \"6g\")\\\n",
    "    .set(\"spark.driver.extraClassPath\" , \"postgresql-42.2.24.jar\")\n",
    "    \n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19aad0a8-a6ed-4a66-a866-88794c4744cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define connection properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c723c18-83a0-437d-b6ee-3bfe56f88082",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Salesforce\n",
    "sf = Salesforce(\n",
    "    username = config['username_sf'],\n",
    "    password = config['password_sf'],\n",
    "    security_token = config['securityToken_sf'],\n",
    "    instance_url = config['authEndPoint_sf']\n",
    ")\n",
    "\n",
    "# Snowflake Properties\n",
    "\n",
    "snow_options = {\n",
    "    \"sfURL\": f\"{config['account']}.snowflakecomputing.com\" ,\n",
    "    \"sfUser\": config['user'] ,\n",
    "    \"sfPassword\": config['password'] ,\n",
    "    \"sfDatabase\": config['database'] ,\n",
    "    \"sfSchema\": config['schema'] ,\n",
    "    \"sfWarehouse\": config['warehouse'] ,\n",
    "    \"sfRole\": config['role']\n",
    "}\n",
    "\n",
    "\n",
    "# Establish a connection to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user = config['user'],\n",
    "    password = config['password'],\n",
    "    account= config['account'],\n",
    "    warehouse= config['warehouse'],\n",
    "    database= config['database'],\n",
    "    schema= config['schema']\n",
    ")\n",
    "\n",
    "\n",
    "# A list of messeages to send to me\n",
    "\n",
    "msg_to_send = []\n",
    "Schema_change_vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be4b8ca4-e099-4156-b893-2ecd50e9b1dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[REDACTED]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow_options[\"sfSchema\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1159cce-a014-4e19-a94d-c32776155420",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54df2c5-11fd-4e7f-8f1b-dcbc2e85ad6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dummy_json_str = {\n",
    "    \"_id\": \"6138763984fe5172962f8147\",\n",
    "    \"balance\": \"$3,677.47\",\n",
    "    \"about\": \"Lorem ipsum Aliqua ad elit elit veniam in mollit officia\",\n",
    "    \"registered\": \"2018-05-04T02:28:05 -02:00\",\n",
    "    \"latitude\": 60.763774\n",
    "}\n",
    "\n",
    "dummy_json = json.dumps(dummy_json_str)\n",
    "dummy_array = ['Lorem Ipsum']#dummy_json\n",
    "dummy_array = array(lit(dummy_array[0]))\n",
    "\n",
    "dummy_mail = 'dummy@mail.com'\n",
    "dummy_string = 'Lorem ipsum'\n",
    "dummy_phone = '+000000000000'\n",
    "dummy_ip = '127.0.0.1'\n",
    "dummy_date = '1660-01-01'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3087f004-181b-4967-b6e3-ec27ea5bda5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Main Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce1de82-d03c-4c5b-a2fd-b428529ebb6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd1d53e-9362-48b5-b73b-fbb774b5f994",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Example usage:\\n\\ntext_to_hash = 'world@gmail.com'\\nresult = hash_value(text_to_hash)\\nprint(result)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_email(email):\n",
    "    parts = email.split('@', 1)\n",
    "    return parts[0], \"@\" + parts[1]\n",
    "\n",
    "def is_valid_email(email):\n",
    "    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    match = re.match(email_pattern, email)\n",
    "    return bool(match)\n",
    "\n",
    "def hash_value(text: str) -> str:\n",
    "    if not text:\n",
    "        text = 'null'\n",
    "\n",
    "    if is_valid_email(text):\n",
    "        username, domain = split_email(text)\n",
    "        return hashlib.sha256((username.encode() + config['enc_key'].encode())).hexdigest() + domain\n",
    "    else:\n",
    "        return hashlib.sha256((text.encode() + config['enc_key'].encode())).hexdigest()\n",
    "\n",
    "encrypt_data_udf = udf(hash_value, StringType())\n",
    "\n",
    "\"\"\"\n",
    "# Example usage:\n",
    "\n",
    "text_to_hash = 'world@gmail.com'\n",
    "result = hash_value(text_to_hash)\n",
    "print(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51177def-2ff7-4c03-82b0-c68201bee520",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Anonymize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f7e731-f4f2-4013-8832-c46615a9d467",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def anonymise(pg_df):\n",
    "    if tab == 'contact':\n",
    "\n",
    "        # lastname, firstname, name, phone, fax, mobilephone, email, birthdate, fromname__c, fromemail__c, fromphone__c, frommobile__c\n",
    "\n",
    "        # Apply the encryption UDF to the \"name\" column      \n",
    "        pg_df = pg_df.withColumn(\"lastname\", encrypt_data_udf(col(\"lastname\")))\n",
    "        pg_df = pg_df.withColumn(\"firstname\", encrypt_data_udf(col(\"firstname\")))\n",
    "        pg_df = pg_df.withColumn(\"name\", encrypt_data_udf(col(\"name\")))\n",
    "        pg_df = pg_df.withColumn(\"email\", encrypt_data_udf(col(\"email\")))\n",
    "        pg_df = pg_df.withColumn(\"birthdate\", lit(dummy_date))\n",
    "        pg_df = pg_df.withColumn(\"phone\", lit(dummy_phone))\n",
    "        pg_df = pg_df.withColumn(\"fax\", lit(dummy_phone))\n",
    "        pg_df = pg_df.withColumn(\"mobilephone\", lit(dummy_phone))\n",
    "        pg_df = pg_df.withColumn(\"fromname__c\", encrypt_data_udf(col(\"fromname__c\")))\n",
    "        pg_df = pg_df.withColumn(\"fromemail__c\", encrypt_data_udf(col(\"fromemail__c\")))\n",
    "        pg_df = pg_df.withColumn(\"fromphone__c\", lit(dummy_phone))\n",
    "        pg_df = pg_df.withColumn(\"frommobile__c\", lit(dummy_phone))        \n",
    "        return pg_df  \n",
    "      \n",
    "    else:\n",
    "        return pg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "250f8abb-f7f2-44a5-ba7b-bb405bc30163",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reading data from tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfdabc6e-a6da-4da1-b258-7d4f661a9ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsnow_options[\"sfSchema\"] = \"[REDACTED]_PARALLEL\"\\n\\nconn = snowflake.connector.connect(\\n    user = config[\\'user\\'],\\n    password = config[\\'password\\'],\\n    account= config[\\'account\\'],\\n    warehouse= config[\\'warehouse\\'],\\n    database= config[\\'database\\'],\\n    schema= \\'[REDACTED]_PARALLEL\\' #config[\\'schema\\']\\n)\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment below for DEV\n",
    "\n",
    "\"\"\"\n",
    "snow_options[\"sfSchema\"] = \"SALESFORCE_PARALLEL\"\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user = config['user'],\n",
    "    password = config['password'],\n",
    "    account= config['account'],\n",
    "    warehouse= config['warehouse'],\n",
    "    database= config['database'],\n",
    "    schema= 'SALESFORCE_PARALLEL' #config['schema']\n",
    ")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a1788b-5853-484e-bec0-62014f8d6b32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Determine Salesfoce table field type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de8fbe8-77b1-47f8-abc1-246fadcd5799",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the __get_field_type function to determine the field type\n",
    "def __get_field_type(field_type):\n",
    "    if field_type == \"string\":\n",
    "        return StringType()\n",
    "    elif field_type == \"int\":\n",
    "        return IntegerType()\n",
    "    elif field_type == \"double\":\n",
    "        return DoubleType()\n",
    "    \n",
    "    # Comment the code below if you want to get Salesfore's datetime format instead of standardize datetime format\n",
    "    \n",
    "    elif field_type == \"date\":\n",
    "        return DateType()\n",
    "    elif field_type == \"datetime\":\n",
    "        return TimestampType()\n",
    "    \n",
    "    # Comment the code above if you want to get Salesfore's datetime format instead of standardize datetime format\n",
    "\n",
    "\n",
    "    # Add more conditions for other field types as needed\n",
    "\n",
    "    # Return a default field type if the input field_type is not recognized\n",
    "    return StringType()  # Change this to the appropriate default type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79588ebc-0303-41c2-949b-8afb48777cfb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Reading data from source table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9328a82c-6a67-4693-9d51-e539919120b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Data from Salesforce Object(Table)\n",
    "\n",
    "def read_src_sf_table(table: str, query_extension: str = None ) -> 'DataFrame':\n",
    "    \n",
    "    ordering = 'CreatedDate'\n",
    "    if table == 'userrole':\n",
    "        ordering = 'LASTMODIFIEDDATE'\n",
    "    try:\n",
    "        # Get object description\n",
    "        obj_description = sf.__getattr__(table).describe()\n",
    "\n",
    "        # Read schema\n",
    "        fields = obj_description[\"fields\"]\n",
    "        schema = StructType([StructField(field[\"name\"], __get_field_type(field[\"type\"]), True) for field in fields])\n",
    "\n",
    "        # Query data using SOQL\n",
    "        #print(f\"SELECT {','.join([field['name'] for field in fields])} FROM {table} {query_extension}\")\n",
    "        #query = \"SELECT Id, IsDeleted, AccountId, CreatedById, CreatedDate, Field, DataType, OldValue, NewValue FROM AccountHistory WHERE CreatedDate >= 2023-06-09T15:26:16Z ORDER BY CreatedDate ASC\"\n",
    "\n",
    "        #data = sf.query_all(f\"SELECT {' , '.join([field['name'] for field in fields])} FROM {table} {query_extension} ORDER BY {ordering} ASC LIMIT 100000 \")\n",
    "        data = sf.query_all(f\"SELECT {' , '.join([field['name'] for field in fields])} FROM {table} {query_extension} \")\n",
    "\n",
    "        #data = sf.query_all(query)\n",
    "        #print(f\"Query sent --> {data}\")\n",
    "        records = data[\"records\"]\n",
    "\n",
    "        # Extract rows from the records\n",
    "        rows = []\n",
    "\n",
    "        for record in records:\n",
    "            row = []\n",
    "\n",
    "            for field in fields:\n",
    "                field_name = field[\"name\"]\n",
    "                field_value = record.get(field_name)\n",
    "\n",
    "                # Comment the code below and in the __get_field_type() method above if you want to get/use Salesfore's datetime format instead of standardize datetime format\n",
    "\n",
    "                # Handle date and datetime fields. \n",
    "                if field[\"type\"] == \"date\" and field_value is not None:\n",
    "                    field_value = datetime.strptime(field_value, \"%Y-%m-%d\").date()\n",
    "                elif field[\"type\"] == \"datetime\" and field_value is not None:\n",
    "                    field_value = field_value.replace('T', ' ')[:19]\n",
    "                    field_value = datetime.strptime(field_value, \"%Y-%m-%d %H:%M:%S\") #to_timestamp(field_value, \"yyyy-MM-dd HH:mm:ss \") '%Y-%m-%d %H:%M:%S %z'\n",
    "                    \n",
    "                # Comment the code below and in the __get_field_type() method above if you want to get/use Salesfore's datetime format instead of standardize datetime format\n",
    "\n",
    "                row.append(field_value)\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "        # Convert data to PySpark DataFrame and return\n",
    "        return spark.createDataFrame(rows, schema)\n",
    "         \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error occured -> \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e5a085-6661-4d5b-81da-f8b3e781dcde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndat = read_src_sf_table('accounthistory')\\ndisplay(dat)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "dat = read_src_sf_table('accounthistory')\n",
    "display(dat)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cfa958c-b210-40d1-be49-933e2c2b5977",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Reading data from target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f5e728-f7fb-4dfd-9cff-8bc042b02ce3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Data from Snowflake Database table\n",
    "\n",
    "def read_tgt_snw_table(query: str, snow_options: dict) -> 'DataFrame':\n",
    "    #print(query)\n",
    "    try:\n",
    "        # Read table into a dataframe\n",
    "        snow_table_df = spark.read.format(\"snowflake\").options(**snow_options).option(\"query\", query).load()\n",
    "        return snow_table_df\n",
    "    except Exception as e:\n",
    "        print(\"Error occured -> \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44f7acfe-d996-4d65-acb6-b0522aea08a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Get source database tables to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4566313-c55f-4428-b993-4029b3a14b94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all database tables to migrate\n",
    "\n",
    "def get_all_tabs_to_migrate() -> list:\n",
    "    migrated_tabs = ['account', 'userrole', 'user', 'opportunity', 'deploiement__c', 'deploiement__history', 'accounthistory']\n",
    "    return migrated_tabs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b64dee1-8a09-4493-9c3c-ea673ea74921",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all target database tables\n",
    "def get_all_tgt_tabs_to_migrate(snow_options: dict) -> list:\n",
    "    get_all_tar_tables_snw = f\"(SELECT DISTINCT LOWER(TABLE_NAME) as TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{snow_options['sfSchema']}' AND TABLE_CATALOG = '{snow_options['sfDatabase']}' AND TABLE_TYPE = 'BASE TABLE' ORDER BY LOWER(TABLE_NAME) ASC) all_tgt_tabs\"\n",
    "    all_snw_tabs = read_tgt_snw_table(get_all_tar_tables_snw, snow_options)\n",
    "    all_snw_tabs = all_snw_tabs.select(\"TABLE_NAME\").collect()\n",
    "    return [row[\"TABLE_NAME\"] for row in all_snw_tabs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0282b598-50c1-4533-9680-da8c01ea3d58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[REDACTED]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow_options['sfSchema']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f522ad03-93da-4f12-952f-bd0f57a80d4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Schema change detection and handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6decc4e-fc1a-43bd-9ad1-c6134b1d6f2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Schema change detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d65877d-4894-4a41-8e3e-e6fe747fff43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_schema_difference(table_to_check: str) -> bool: \n",
    "    Schema_change_vals = []\n",
    "    chng_str = \"\"\n",
    "    #print('Table ', table_to_check)\n",
    "    \n",
    "    # Processing Source(Salesforce) table\n",
    "    src_sf_df = read_src_sf_table(table_to_check, ' LIMIT 1')\n",
    "    #print(f\"src_sf_df {src_sf_df}\")\n",
    "\n",
    "    # Get column metadata from Salesforce table\n",
    "    src_sf_metadata = set([(row.name.lower(), str(row.dataType), row.nullable) for row in src_sf_df.schema])\n",
    "    #print(f\"src_sf_metadata -> {src_sf_metadata}\")\n",
    "\n",
    "\n",
    "    # Processing Target(Snowflake) table\n",
    "    tar_snow_query = f\"(SELECT * FROM {table_to_check} LIMIT 1) {table_to_check}\"\n",
    "    tar_snow_df = read_tgt_snw_table(tar_snow_query, snow_options)\n",
    "\n",
    "    # Get column metadata from Snowflake table\n",
    "    tar_snow_metadata = set( [(row.name.lower(), str(row.dataType), row.nullable) for row in tar_snow_df.schema])\n",
    "    #print(f\"\\ntar_snow_metadata -> {tar_snow_metadata}\\n\")    \n",
    "\n",
    "    # Definition of data type mapping between Postgres and Snowflake\n",
    "    data_type_mapping = {\n",
    "        'LongType()': 'DecimalType(38,0)',\n",
    "        'bigint': 'DecimalType(38,0)',\n",
    "        'IntegerType()': 'DecimalType(38,0)',\n",
    "        'ArrayType(StringType(), True)': 'StringType()',\n",
    "        'ShortType()': 'DecimalType(38,0)'\n",
    "    }\n",
    "\n",
    "    # Apply data type mapping to Source (Postgres) metadata\n",
    "    mapped_src_sf_metadata = set(list([(column_name.lower(), data_type_mapping.get(data_type, data_type), is_nullable) for column_name, data_type, is_nullable in src_sf_metadata]))    \n",
    "    #print(f\"\\nmapped_src_sf_metadata --> {mapped_src_sf_metadata}\\n\")\n",
    "\n",
    "    # Compare schemas\n",
    "    schema_diff = mapped_src_sf_metadata - tar_snow_metadata\n",
    "    print(f\"schema_diff -> {schema_diff}\")    \n",
    "    if len(schema_diff) > 0 and len(src_sf_df.columns) != len(tar_snow_df.columns):\n",
    "        #print(\"There's a schema change\")\n",
    "        for column in schema_diff:\n",
    "            column_name, data_type, is_nullable = column\n",
    "            #print(f\"Column: {column_name}, Data Type: {data_type}, Nullable: {is_nullable}\")\n",
    "            chng_str += f\"Column: {column_name}, Data Type: {data_type}, Nullable: {is_nullable}\\n\"\n",
    "        chng_str = f\"Database Table {table_to_check}\\n\"  + chng_str\n",
    "        Schema_change_vals.append(chng_str)\n",
    "        return True\n",
    "    else:\n",
    "        #print(\"There's no schema change\")\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c086b18-f69e-4f86-90ad-594936ce784e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#check_schema_difference('AccountHistory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f8cb203-ca8b-462d-b268-0c570deacbf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "973c04b8-1f9e-40f7-a50c-b1a4c177ead7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Schema change handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db827f6f-ba6a-4020-b47d-c7e72b7eab23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def handle_schema_change(data_df: 'DataFrame', table_name: str, versioning_schema: str) -> None:\n",
    "    try:\n",
    "        # Get current timestamp\n",
    "        current_timestamp = str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        temp_new_table = table_name+'__'+current_timestamp\n",
    "        print(temp_new_table)\n",
    "\n",
    "        # Save table with new name\n",
    "        data_df.write.format(\"snowflake\").options(**snow_options).option(\"dbtable\", temp_new_table).mode(\"append\").save() # \"overwrite\" \"append\"\n",
    "\n",
    "        # Updates: \n",
    "                # -> Move old table to alternate schema and rename it by appending the current_timestamp of the new table to its name\n",
    "                # -> Rename the new table to the name of the old table by stripping the current_timestamp from it\n",
    "\n",
    "        # Start a transaction\n",
    "        conn.cursor().execute(\"BEGIN TRANSACTION\")\n",
    "\n",
    "        try:\n",
    "            # Send old table to temp schema and rename it\n",
    "            conn.cursor().execute(f\"ALTER TABLE {snow_options['sfDatabase']}.{snow_options['sfSchema']}.{table_name} RENAME TO {snow_options['sfDatabase']}.{versioning_schema}.{temp_new_table}\")\n",
    "\n",
    "            # Rename new table to old table\n",
    "            conn.cursor().execute(f\"ALTER TABLE {snow_options['sfDatabase']}.{snow_options['sfSchema']}.{temp_new_table} RENAME TO {snow_options['sfDatabase']}.{snow_options['sfSchema']}.{table_name}\")\n",
    "\n",
    "            # Commit the transaction\n",
    "            conn.cursor().execute(\"COMMIT\")\n",
    "            msg = f\"Schema change detected and new table created at {temp_new_table}\"\n",
    "\n",
    "            msg_to_send.append(msg)\n",
    "        except Exception as e:\n",
    "            # Rollback the transaction in case of any error\n",
    "            conn.cursor().execute(\"ROLLBACK\")\n",
    "            print(\"Rolling Back...\\nError occurred -> \", e)    \n",
    "\n",
    "\n",
    "        #return saved_df\n",
    "    except Exception as e:\n",
    "        print(\"Error occured -> \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768c3d36-ec21-4193-ab1d-afb085e43e55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d4e99e1-3113-477e-973a-69972a50e316",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Saving Data to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4f58ada-fb9e-4238-82ff-cbe9da1ad7dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### UPSERT target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5855cc3f-8b9a-4115-804f-e0e584f71905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method essentially updates (MERGE) the target table\n",
    "\"\"\"\n",
    "\n",
    "def upsert_target_table(tab: str, merge_key: str = None) -> bool:\n",
    "\n",
    "    if merge_key is None:\n",
    "        merge_key = 'id'\n",
    "\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"BEGIN TRANSACTION\")\n",
    "\n",
    "    try:\n",
    "        ## Get the column names of the target table\n",
    "        target_columns_query = f\"SHOW COLUMNS IN {snow_options['sfDatabase']}.{snow_options['sfSchema']}.{tab}\" \n",
    "        cursor.execute(target_columns_query)\n",
    "        column_names = [column[2] for column in cursor.fetchall()]\n",
    "\n",
    "        # Generate the column mappings for the MERGE query, handling reserved keywords\n",
    "        column_mappings = \", \".join([f\"\\\"target\\\".\\\"{col}\\\" = \\\"source\\\".\\\"{col}\\\"\" for col in column_names])\n",
    "        #print(f\"column_mappings {column_mappings}\")\n",
    "\n",
    "        # Perform the MERGE operation to insert new rows or update existing rows\n",
    "        #print(f\"Upserting.... snow_options['sfSchema'] {snow_options['sfSchema']}\")\n",
    "        merge_query = (\n",
    "            \"MERGE INTO \\\"\" + snow_options['sfDatabase']+\"\\\".\\\"\" + snow_options['sfSchema']+\"\\\".\"+\"\\\"\"+tab.upper() + \"\\\" AS \\\"target\\\" \"\n",
    "            \"USING \" + snow_options['sfDatabase']+\".\" + snow_options['sfSchema']+\".TEMP_TABLE AS \\\"source\\\" \"\n",
    "            \"ON \\\"target\\\".\\\"\" + merge_key + \"\\\" = \\\"source\\\".\\\"\" + merge_key + \"\\\" \"\n",
    "            \"WHEN MATCHED THEN \"\n",
    "            \"UPDATE SET \" + column_mappings + \" \"\n",
    "            \"WHEN NOT MATCHED THEN \"\n",
    "            \"INSERT (\" + \", \".join([\"\\\"\" + col + \"\\\"\" for col in column_names]) + \") \"\n",
    "            \"VALUES (\" + \", \".join([\"\\\"source\\\".\\\"\" + col + \"\\\"\" for col in column_names]) + \")\"\n",
    "        )\n",
    "        #print(f\"\\nQuery to be executed {merge_query}\")\n",
    "        #print(snow_options['sfRole'])\n",
    "        cursor.execute(merge_query)\n",
    "        #print(f\"\\nQuery executed\")\n",
    "        \n",
    "        # Delete the previously created TEMP_TABLE to free up the database\n",
    "        cursor.execute(f\"DROP TABLE {snow_options['sfDatabase']}.{snow_options['sfSchema']}.TEMP_TABLE\")\n",
    "\n",
    "        print(f\"\\nDrop table Query executed\")\n",
    "\n",
    "        # Commit the changes and close the connection\n",
    "        cursor.execute(\"COMMIT\")  \n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        #print('Some error occured')\n",
    "        # Rollback the transaction in case of any error    \n",
    "        msg_to_send.append(f\"Could not Merge UPDATE table {tab} because of the following error\\n{e}\\n, and consequently the table was not migrated.\\n Go take a look\")\n",
    "        cursor.execute(\"ROLLBACK\")\n",
    "        print(\"Rolling Back...\\nError occurred -> \", e)  \n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f32216d7-f609-4a1b-9c85-34787c01d585",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Saving data to target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80378f8-1e52-44cb-9476-b496718f352c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save Data to Snowflake Database table\n",
    "\n",
    "def load_tgt_snw_table(data_df: 'DataFrame', database_table: str, snow_options: dict, write_mode: str = None, updating: str = None) -> 'DataFrame':\n",
    "    try:\n",
    "        old_tab = database_table # Keep the table name just in case\n",
    "\n",
    "        # Default write method\n",
    "        if write_mode is None:\n",
    "            write_mode = \"append\"\n",
    "        \n",
    "        # If updating, save data instead in a TEMP_TABLE for later MERGE operation and then change write mode to overite\n",
    "        if updating is not None:\n",
    "            database_table = \"TEMP_TABLE\"\n",
    "            write_mode = \"overwrite\"\n",
    "\n",
    "        # Read table into a dataframe\n",
    "        print('About to save')\n",
    "        data_df.write.format(\"snowflake\").options(**snow_options).option(\"dbtable\", database_table).mode(f\"{write_mode}\").save()\n",
    "        print('saved')\n",
    "        if updating is not None:\n",
    "            print('UPDATE ')\n",
    "            upsert_target_table(old_tab)\n",
    "\n",
    "        #return saved_df\n",
    "    except Exception as e:\n",
    "        print(\"Error occured -> \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4e6b9ab-6cdd-4033-9ecc-9b2cfc7a1a93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sending Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536739dd-0d5e-4a03-86cf-c9c8a8a738bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def send_mail(subject: str, message: str, any_link: str=None) -> None:\n",
    "    import smtplib\n",
    "    from email.mime.text import MIMEText\n",
    "    from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "    if any_link is None:\n",
    "        any_link = 'No link'\n",
    "\n",
    "    \n",
    "    username = config['username_mail'] \n",
    "    password = config['password_mail']\n",
    "    msg = MIMEMultipart('mixed')\n",
    "\n",
    "    sender = 'my_email@mail.com'\n",
    "    recipient = 'my_email@mail.com'\n",
    "\n",
    "    msg['Subject'] = subject \n",
    "    msg['From'] = sender\n",
    "    msg['To'] = recipient\n",
    "    \n",
    "    text_message = MIMEText(str(message))\n",
    "    html_message = MIMEText(f\"<br><b>File Located at:</b> <a href='#'>{any_link}</a>\", 'html')\n",
    "    msg.attach(text_message)\n",
    "    msg.attach(html_message)\n",
    "\n",
    "    mailServer = smtplib.SMTP('mail.smtp2go.com', 2525) # 8025, 587 and 25 can also be used.\n",
    "    mailServer.ehlo()\n",
    "    mailServer.starttls()\n",
    "    mailServer.ehlo()\n",
    "    mailServer.login(username, password)\n",
    "    mailServer.sendmail(sender, recipient, msg.as_string())\n",
    "    mailServer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e5bcf2b-2526-4356-869c-8e70a356867c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a5a0397-7e87-4da3-9f8f-c778a6958f64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Testing DB Connections and geting source and target tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74b5a40f-667c-4952-8f6b-7ffa97d2b62e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source tables ->  ['account', 'userrole', 'user', 'opportunity', 'deploiement__c', 'deploiement__history', 'accounthistory'] \n",
      "\n",
      " of length ->  7 \n",
      "-----\n",
      "\n",
      "\n",
      "Source tables ->  ['account', 'accountcontactrelation', 'accounthistory', 'accountteammember', 'asset', 'case', 'casefeed', 'contact', 'deploiement', 'deploiement__c', 'deploiement__history', 'emailmessage', 'emailmessagerelation', 'event', 'knowledgearticle', 'kyc', 'lead', 'leadhistory', 'opportunity', 'opportunitylineitem', 'order_c', 'orderitem', 'profile', 'role', 'sf_users', 'sf_zoom_call_log', 'task', 'user', 'userrole', 'zoom_app__zoom_call_log__c'] \n",
      "\n",
      " of length ->  30\n"
     ]
    }
   ],
   "source": [
    "all_sf_tabs = get_all_tabs_to_migrate()\n",
    "print(\"Source tables -> \", all_sf_tabs, '\\n\\n of length -> ', len(all_sf_tabs), '\\n-----')\n",
    "\n",
    "all_snw_tabs = get_all_tgt_tabs_to_migrate(snow_options)\n",
    "print(\"\\n\\nSource tables -> \", all_snw_tabs, '\\n\\n of length -> ', len(all_snw_tabs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c329d5-ef12-4af4-bbd2-f367ff6b787a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d61d22bf-a23e-4467-9098-ba3e4d29850b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Core Migration method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de545193-dbbb-41d8-a02b-a79828a29287",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "933c717c-05de-4fd4-a92f-052fc19b2205",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Now processing table account...\n",
      "schema_diff -> set()\n",
      "get max and proceed\n",
      "max_date  2023-07-26T13:00:15Z  Type  <class 'str'>\n",
      "About to save\n",
      "saved\n",
      "UPDATE \n",
      "\n",
      "Drop table Query executed\n",
      "Now processing table accountcontactrelation...\n",
      "schema_diff -> set()\n",
      "get max and proceed\n",
      "max_date  2023-07-26T13:04:10Z  Type  <class 'str'>\n",
      "About to save\n",
      "saved\n",
      "UPDATE \n",
      "\n",
      "Drop table Query executed\n",
      "Now processing table accounthistory...\n",
      "schema_diff -> set()\n",
      "get max and proceed\n",
      "max_date  2023-07-26T13:05:11Z  Type  <class 'str'>\n",
      "About to save\n",
      "saved\n",
      "UPDATE \n",
      "\n",
      "Drop table Query executed\n",
      "Now processing table accountteammember...\n",
      "schema_diff -> set()\n",
      "get max and proceed\n",
      "max_date  2023-07-26T13:05:09Z  Type  <class 'str'>\n",
      "About to save\n",
      "saved\n",
      "UPDATE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "salesforce_tables = get_all_tabs_to_migrate()\n",
    "all_snw_tabs = get_all_tgt_tabs_to_migrate(snow_options)\n",
    "\n",
    "deleted_salesforce_objects = ['deploiement', 'kyc', 'order_c', 'role', 'sf_users', 'sf_zoom_call_log']\n",
    "print(len(salesforce_tables))\n",
    "all_snw_tabs = sorted(list(set(all_snw_tabs) - set(deleted_salesforce_objects)))\n",
    "all_snw_tabs.remove('leadhistory')\n",
    "all_snw_tabs.append('leadhistory')\n",
    "no_created_at = []\n",
    "all_snw_tabs\n",
    "\n",
    "for tab in all_snw_tabs: # salesforce_tables\n",
    "    snow_options['sfWarehouse'] = 'COMPUTE_WH'\n",
    "    print(f\"Now processing table {tab}...\")\n",
    "\n",
    "    max_col = 'CreatedDate'\n",
    "    if tab == 'userrole':\n",
    "        max_col = 'LASTMODIFIEDDATE'\n",
    "    \n",
    "    \"\"\"\n",
    "    if tab not in salesforce_tables: #all_snw_tabs:        \n",
    "        sf_df = read_src_sf_table(tab)\n",
    "        #display(sf_df)\n",
    "        snow_options['sfWarehouse'] = 'COMPUTE_WH'\n",
    "        load_tgt_snw_table(sf_df, tab, snow_options, \"overwrite\")\n",
    "    #msg_to_send.append(f\"New table {tab} migrated\")\n",
    "    continue\n",
    "    \"\"\"\n",
    "\n",
    "    if check_schema_difference(tab):\n",
    "        print('handle')\n",
    "        sf_df = read_src_sf_table(tab)\n",
    "        sf_df = anonymise(sf_df)\n",
    "        handle_schema_change(sf_df, tab, 'TEMP_HOLD')\n",
    "    else:\n",
    "        print('get max and proceed')\n",
    "\n",
    "        if tab in no_created_at:\n",
    "\n",
    "            # select all and overwrite          \n",
    "            sf_df = read_src_sf_table(tab)\n",
    "            sf_df = anonymise(sf_df)\n",
    "            load_tgt_snw_table(sf_df, tab, snow_options, \"overwrite\")\n",
    "        else:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(f\"SELECT MAX({max_col}) FROM {snow_options['sfDatabase']}.{snow_options['sfSchema']}.{tab} \")\n",
    "            max_date = cursor.fetchone()[0]\n",
    "            datetime_obj = datetime.strptime(str(max_date), \"%Y-%m-%d %H:%M:%S\")\n",
    "            max_date = datetime_obj.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            #max_date = datetime.datetime.strptime(str(max_date), '%Y-%m-%d %H:%M:%S').isoformat()\n",
    "            print('max_date ', max_date, ' Type ', type(max_date))\n",
    "            sf_query = f\" WHERE {max_col} >= {max_date} ORDER BY {max_col} ASC \" # ORDER BY CreatedDate ASC   OR LastModifiedDate >= {max_date} \n",
    "            sf_df = read_src_sf_table(tab, sf_query)\n",
    "            sf_df = anonymise(sf_df)\n",
    "\n",
    "            load_tgt_snw_table(sf_df, tab, snow_options, \"overwrite\", \"update\") #, \"update\"\n",
    "\n",
    "\n",
    "#print(msg_to_send)\n",
    "if len(msg_to_send) > 0:\n",
    "    any_link = 'https://databricks_notebook_URL'\n",
    "    sbj = 'Salesforce ETL - job from Databrick! '\n",
    "    final_msg = ''.join([f\"{i}\\n\" for i in msg_to_send])\n",
    "\n",
    "    send_mail(sbj, final_msg, any_link)            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4ce6a2-1091-488d-9340-8b3164d03e53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "tab = 'task'\n",
    "sf_df = read_src_sf_table(tab)\n",
    "#display(sf_df)\n",
    "snow_options['sfWarehouse'] = 'COMPUTE_WH'\n",
    "load_tgt_snw_table(sf_df, tab, snow_options, \"overwrite\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f72c189-32f2-4dfb-803b-632b4c3b1f3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Delete all below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60989ba0-31b6-4493-8958-44dce902b5da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_msg = []\n",
    "msg_to_send = []\n",
    "final_msg, msg_to_send"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c6548e-9a6e-46f6-8e1f-d18c63b49aa4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### End Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed62ed6-661e-4602-be36-4b418642cd93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the code below after DEV\n",
    "#spark.stop()\n",
    "\n",
    "# Close the connection\n",
    "#cursor.close()\n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3df01afd-4817-4397-b162-34c1bb785e0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "5582d75e-19d0-4d17-99b9-d74c349edece",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "6638cd47-8847-4803-ae0d-566341c948be",
     "origId": 2033082681461162,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Salesforce_Migration_Job",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
